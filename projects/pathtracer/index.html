<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Pathtracer | George Geng </title> <meta name="author" content="George Geng"> <meta name="description" content="Implemented the core routines of a physically-based renderer using the pathtracing algorithm."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/favicon.png?98ab68992a4209ac66024f789453914e"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://jorjboi.github.io/projects/pathtracer/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">George</span> Geng </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/art/">art </a> </li> <li class="nav-item "> <a class="nav-link" href="/papers/">papers </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Pathtracer</h1> <p class="post-description">Implemented the core routines of a physically-based renderer using the pathtracing algorithm.</p> </header> <article> <ul> <li><a href="#overview">Overview</a></li> <li><a href="#ray-generation-and-intersection">Ray Generation and Intersection</a></li> <li><a href="#bounding-volume-hierarchies">Bounding Volume Hierarchies</a></li> <li><a href="#direct-illumination">Direct Illumination</a></li> <li><a href="#global-illumination">Global Illumination</a></li> <li><a href="#adaptive-sampling">Adaptive Sampling</a></li> <li><a href="#reflective-and-refractive-bsdfs">Reflective and Refractive BSDFs</a></li> <li><a href="#microfacet-brdf">Microfacet BRDF</a></li> <li><a href="#thin-lens-and-autofocus">Thin Lens and Autofocus</a></li> </ul> <h2 id="overview">Overview</h2> <p>This project was done for CS184 Computer Graphics &amp; Imaging at UC Berkeley, taught by Professor Ren Ng. In this project, I implemented the core functionality of a physically-based renderer and the rendering equation.</p> <p>In path tracing, rays are traced from the camera through each pixel of an image into a three-dimensional scene, where they encounter different surfaces.</p> <p>At each intersection with a surface, the outgoing light towards the camera is computed using the total incoming light and the Bidirectional Scattering Function (BSDF) of that surface. The incoming light that reaches that point is found by tracing the ray recursively back through the scene, and accumulating the light reflected or transmitted by different surfaces (or light sources) at each subsequent intersection with a surface.</p> <p>By integrating over all the light arriving at a point and scaling it by the BSDF, we can determine the final amount of radiance towards the camera and compute the color of the pixel.</p> <p>This process is repeated for every pixel to form an entire image.</p> <hr> <h2 id="ray-generation-and-intersection">Ray Generation and Intersection</h2> <p>Imagine a virtual camera looking at scene; each pixel of the final image has a corresponding location on that virtual camera sensor.</p> <p>First, we convert the pixel’s location from image space to camera space:</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" sizes="95vw"></source> <img src="/assets/img/pathtracer/raycasting.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="raycasting" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>Then, we can define a ray that starts at the camera’s origin and goes through the location of that pixel on the virtual camera sensor, like a little hole in a mesh screendoor. Note that rays should exist in world space–the world of the scene.</p> <p>Since pixels are specified by the coordinates just one of their corners, we generate a bunch of rays over the unit square of each pixel by random sampling (each ray has an ever-so-slightly different direction).</p> <p>For each ray, we need to check for an intersection with a surface in the scene. We can see if a ray and a triangle intersect using the <strong>Möller-Trumbore</strong> algorithm:</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" sizes="95vw"></source> <img src="/assets/img/pathtracer/moller_trumbore.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="moller trumbore" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>This algorithm takes a ray with origin <code class="language-plaintext highlighter-rouge">O</code> and direction <code class="language-plaintext highlighter-rouge">D</code> and tests for its intersection with a triangle with vertices <em>P<sub>0</sub></em> , <em>P<sub>1</sub></em> , and <em>P<sub>2</sub></em> . If the barycentric coordinates <em>b<sub>1</sub></em> , <em>b<sub>2</sub></em> , and <em>1 - b<sub>1</sub> - b<sub>2</sub></em> specify a point within the triangle, then there is an intersection.</p> <p>However, testing against every triangle in a scene for an intersection is costly and inefficient, so I accelerate this process by constructing a bounding volume hierarchy for the scene.</p> <hr> <h2 id="bounding-volume-hierarchies">Bounding Volume Hierarchies</h2> <p>A bounding volume hierarchy (BVH) is a tree in which each node is the bounding box of a collection of primitives in a mesh, and the leaves are the individual primitives themselves (triangles). Instead of testing for an intersection with every primitive in a mesh, we can check for intersection with bounding boxes, and only check for intersection against individual primitives when we hit a leaf.</p> <p>At each layer of BVH construction, I choose to split the primitives into left or right branches of the BVH using their <strong>average centroid</strong> as the split point.</p> <p>The BVH is constructed recursively. The intersection test for each ray improves from linear to logarithmic time, so now we can quickly generate images of complicated meshes with thousands of triangles:</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" sizes="95vw"></source> <img src="/assets/img/pathtracer/max_planck.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="max planck" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Max Planck, rendered with debug normal shading </div> <hr> <h2 id="direct-illumination">Direct Illumination</h2> <p>First, I implemented <strong>direct illumination</strong>. A point on a surface is visible if any rays reflected from the surface directly intersect a light source.</p> <p>For Lambertian surfaces, reflection is equally diffuse in all directions around the point of intersection. Upon finding an intersection with a surface, we can sample the direction of reflected rays using <strong>uniform hemisphere sampling</strong>, where we choose a random direction within a hemisphere centered around the surface normal at the point of intersection.</p> <p>We generate a ray that starts at the point of intersection and trace it in the direction to check if it intersects a light source. Using the amount of incoming light from the source, we can compute the outgoing light using the reflectance equation:</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" sizes="95vw"></source> <img src="/assets/img/pathtracer/reflectance_equation.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="reflectance equation" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> The reflectance equation </div> <p>The total outgoing emission at point <code class="language-plaintext highlighter-rouge">p</code> in the direction <code class="language-plaintext highlighter-rouge">wr</code> is <code class="language-plaintext highlighter-rouge">Lr(p, wr)</code>, and the integral of the total emission coming from every outgoing direction <code class="language-plaintext highlighter-rouge">wi</code> is over a hemisphere <em>H<sup>2</sup></em>.</p> <p><code class="language-plaintext highlighter-rouge">Li(p, wi)</code> is the emission from a light source that a ray starting at <code class="language-plaintext highlighter-rouge">p</code> with direction <code class="language-plaintext highlighter-rouge">wi</code> encounters, <code class="language-plaintext highlighter-rouge">fr(p, wi -&gt; wr)</code> is the evaluation of the BSDF at point <code class="language-plaintext highlighter-rouge">p</code> of a ray being reflected from <code class="language-plaintext highlighter-rouge">wi</code> to <code class="language-plaintext highlighter-rouge">wr</code> towards the camera, and <code class="language-plaintext highlighter-rouge">cos(θi)</code> is used to attenuate the amount of light coming in from an angle.</p> <p>In practice, we approximate this with a Monte Carlo estimator with <code class="language-plaintext highlighter-rouge">N</code> samples and the probability distribution function (for a hemisphere, this is 1/2<em>pi</em> ) :</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" sizes="95vw"></source> <img src="/assets/img/5.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Monte Carlo estimator </div> <p>However, the renders we get back are noisy because many sampled rays never reach a light source at all. While this image would eventually converge, we would need way too many samples. Additionally, sampling uniformly on a hemisphere is unable to practically sample <strong>point light sources</strong>.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" sizes="95vw"></source> <img src="/assets/img/5.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Cornell box bunny: direct illumination, uniform hemisphere sampling </div> <p>A better solution is to use <strong>importance sampling</strong> of lights, where we sample rays in the directions of our scene lights instead of randomly over a hemisphere. A point is directly illuminated only if the rays do not intersect any other surface before reaching the light source.</p> <p>We perform this sampling process for every light source in the scene and sum together the radiance contribution from each light.</p> <p>Now, the renders are far less noisy because they take far fewer samples to converge.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" sizes="95vw"></source> <img src="/assets/img/pathtracer/importance.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Cornell box bunny: direct illumination, importance sampling </div> <hr> <h2 id="global-illumination">Global Illumination</h2> <p>In the real world, objects are illuminated not only by direct light sources, but also indirectly by light reflected or transmitted by other objects in the environment.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" sizes="95vw"></source> <img src="/assets/img/pathtracer/global_illumination.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Global illumination </div> <p>To simulate this physical process, I recursively trace each ray back through the scene until it either intersects a light source or we choose to terminate the ray. We can terminate the ray based on <strong>Russian Roulette</strong>, an unbiased method of random termination, as well as by setting a max number of bounces.</p> <p>The contribution of later bounces to the final pixel radiance decreases exponentially because it should be attenuated by the BSDF at each surface along the way. At first, I forgot to consider that and the renders came out far too bright. An example of a buggy render:</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" sizes="95vw"></source> <img src="/assets/img/pathtracer/CBbunny_16_8_wrong_radiance_no_math.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Cornell box bunny: incorrect radiances </div> <p>Upon fixing this bug, I now have global illumination. We can see some of the color bleeding from the red and blue walls on the right and left sides of the model:</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" sizes="95vw"></source> <img src="/assets/img/pathtracer/CBbunny_16_8_importance_sampling.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Cornell box bunny: global illumination </div> <p>One way to reduce the noise of the images is to increase the number of rays sampled per pixel. At 1024 rays per pixel, the noise is almost impossible to notice.</p> <div class="row justify-content-sm-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" sizes="95vw"></source> <img src="/assets/img/pathtracer/sphere_16.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" sizes="95vw"></source> <img src="/assets/img/pathtracer/sphere_1024.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> 16 vs. 1024 samples per pixel </div> <p>However, taking many samples is a slow and computationally expensive process. We can speed up our renders with adaptive sampling.</p> <hr> <h2 id="adaptive-sampling">Adaptive Sampling</h2> <p>With adaptive sampling, we check if a pixel’s value has converged early as we trace rays through it, and stop sampling if it has.</p> <p>We can check a pixel’s convergence after each batch of <code class="language-plaintext highlighter-rouge">n</code> samples to see if it’s within a 95% confidence interval. Areas of the scene that are shadowed or not directly illuminated take longer to converge. Below are images depicting sample rates for Cornell box spheres and bunny: red indicates that we took more samples to converge, while blue and green indicate that the pixel value converged quickly and we stopped sampling earlier.</p> <div class="row justify-content-sm-center"> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" sizes="95vw"></source> <img src="/assets/img/6.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm-6 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" sizes="95vw"></source> <img src="/assets/img/11.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Blue indicates pixel value converged early; red indicates slow convergence </div> <p>And finally, here is the bunny render, using 1024 samples:</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" sizes="95vw"></source> <img src="/assets/img/5.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h2 id="reflective-and-refractive-bsdfs">Reflective and Refractive BSDFs</h2> <p>So far, I have only modeled Lamberian materials with diffuse reflection. To simulate other materials like metal or glass, I implement reflective and refractive BSDFs.</p> <p>For a perfectly reflective mirror-like surface, the angle between the incoming incident ray and the surface normal is equal to the angle between the surface normal and the outgoing ray.</p> <p>For refractive surfaces, the direction of the transmitted ray is modeled by <strong>Snell’s Law</strong>. When modeling refraction, we also consider the edge case of total internal reflection, when a light ray is internally reflected instead of crossing the boundary between two mediums.</p> <p>We need a minimum ray depth of 2 to have reflections from not only direct light sources but also the surrounding environment:</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" sizes="95vw"></source> <img src="/assets/img/pathtracer/placeholder.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> ray depth = 2 </div> <p>We need a minimum ray depth of 3 to have refraction (the light must first enter the sphere and also exit the sphere before reaching the environment). Note the caustic effect in the bottom right corner, a sign of refraction. At a ray depth of 4, light refracted through the glass sphere on the right can be reflected off the mirror sphere on the left, and we can see its reflection as well.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" sizes="95vw"></source> <img src="/assets/img/pathtracer/placeholder.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> ray depth = 4 </div> <p>The sphere on the right is purely refractive right now. More realistic glass exhibits both refraction and reflection. The ratio of reflection to refraction is modeled by the Fresnel equations in physics, but I model this with <strong>Shlick’s approximation</strong> to make it easier to evaluate. Now, we can see the reflection of the ceiling light on the glass ball on the right as well.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" sizes="95vw"></source> <img src="/assets/img/pathtracer/placeholder.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> ray depth = 4 </div> <h2 id="microfacet-brdf">Microfacet BRDF</h2> <p>We can also model microfacet materials–those with a rough reflective surface such as brushed metal—with a microfacet BRDF. This implementation closely follows the method outlined in this article. Specifically, we use this equation to evaluate the BRDF:</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" sizes="95vw"></source> <img src="/assets/img/pathtracer/equation.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>Where <code class="language-plaintext highlighter-rouge">F</code> is the fresnel term, <code class="language-plaintext highlighter-rouge">G</code> the shadow-masking term, <code class="language-plaintext highlighter-rouge">D</code> the normal distribution function, <code class="language-plaintext highlighter-rouge">n</code> the normal at the macro level, and <code class="language-plaintext highlighter-rouge">h</code> the half-vector (the normal at the microsurface).</p> <p>For <code class="language-plaintext highlighter-rouge">D(h)</code>, I implement the Beckmann distribution, which is more suited to microfacet materials than the Gaussian distribution:</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" sizes="95vw"></source> <img src="/assets/img/pathtracer/equation.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>Here, <code class="language-plaintext highlighter-rouge">α</code> represents the albedo, or the reflectance of a material. We use this to control the glossiness or reflectance of a material. Below is a render of a dragon with a gold material and albedo <code class="language-plaintext highlighter-rouge">α = 0.05</code>.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" sizes="95vw"></source> <img src="/assets/img/pathtracer/dragon.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> α = 0.05 </div> <h2 id="thin-lens-and-autofocus">Thin Lens and Autofocus</h2> <p>Finally, I can extend the path tracer’s virtual camera, which follows an ideal pinhole model, to use a thin lens instead. With a pinhole model, everything is in perfect focus. However, lenses in real cameras and human eyes have a finite aperture and focal length.</p> <p>I first create lens elements that refract the rays first in accordance with <strong>Snell’s Law</strong>, to create depth of field. Next, I implement contrast-based autofocus with a focus metric based on the sum of the variances of an image patch to focus on. By stepping through the depths between my infinity-focus and near-focus positions, I can find the location of the virtual camera sensor that maximizes the amount of contrast in the region I want to be in focus.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" sizes="95vw"></source> <img src="/assets/img/pathtracer/dragon.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> graph </div> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" sizes="95vw"></source> <img src="/assets/img/pathtracer/dragon.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> autofocus </div> <p>Using a double Gauss lens model, the most common type of camera lens, I can focus on this region of the dragon’s head.</p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" sizes="95vw"></source> <img src="/assets/img/pathtracer/dragon.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="example image" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> dragon head </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?70d799092f862ad98c7876aa47712e20"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>